{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9cee863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is for GRF, socio variables\n",
    "# Take Dataset3 NYC for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c61530df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ac6a9",
   "metadata": {},
   "source": [
    "# Geographical RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9590cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeographicalRandomForest:\n",
    "    # this is the initialization function\n",
    "    # param local_model_num controls how many local models will participate in prediction; default is 1 !!!New\n",
    "    def __init__(self, ntree, mtry, band_width, local_weight, local_model_num=1, bootstrap=False, random_seed=42):\n",
    "        self.ntree = ntree\n",
    "        self.mtry = mtry\n",
    "        self.band_width = band_width\n",
    "        self.local_weight = local_weight\n",
    "        self.local_model_num = local_model_num\n",
    "        self.bootstrap=bootstrap\n",
    "        self.random_seed = random_seed\n",
    "        self.global_model = None\n",
    "        self.local_models = None\n",
    "        self.train_data_coords = None\n",
    "        self.distance_matrix = None\n",
    "        self.train_data_index = None\n",
    "        self.train_data_columns = None\n",
    "       \n",
    "    \n",
    "    # param X_train contains a data frame of the the training indepdent variables \n",
    "    # param y_train contains a data series of the target dependent variable\n",
    "    # param coords contains a data frame of the two-dimensional coordinates\n",
    "    # param record_index contains a data series of the indices of the data for helping store local models\n",
    "    def fit(self, X_train, y_train, coords, record_index):\n",
    "        \n",
    "        # save the index of the training data\n",
    "        self.train_data_index = record_index\n",
    "        self.train_data_columns = X_train.columns\n",
    "        \n",
    "        # get Global RF model and importance information, and save global RF model\n",
    "        rf_global = RandomForestRegressor(bootstrap = self.bootstrap, n_estimators = self.ntree, max_features = self.mtry, random_state = self.random_seed) \n",
    "        rf_global.fit(X_train, y_train)\n",
    "        self.global_model = rf_global\n",
    "        \n",
    "        \n",
    "        # create an empty dictionary for local models\n",
    "        self.local_models = {}\n",
    "        \n",
    "        # get the distance matrix between the training geographic features\n",
    "        coords_array = np.array(coords, dtype = np.float64) # translate (x,y) to array type\n",
    "        self.train_data_coords = coords_array\n",
    "        self.distance_matrix = distance.cdist(coords_array,coords_array, 'euclidean') # calculate Euclidean Distance\n",
    "        \n",
    "        # train local models\n",
    "        for i in range(len(X_train)):\n",
    "            distance_array = self.distance_matrix[i]\n",
    "            idx = np.argpartition(distance_array, self.band_width)  # Get the index of the geographic features that are the nearest to the target geographic feature\n",
    "            idx = idx[:self.band_width]  # only those indices within the band_width are valid \n",
    "            \n",
    "            local_X_train = X_train.iloc[idx]\n",
    "            local_y_train = y_train.iloc[idx]\n",
    "            \n",
    "            # make local tree size smaller, because there is no sufficient data to train a big tree !!!New\n",
    "            local_tree_size = int(self.ntree * (self.band_width*1.0/len(X_train)))\n",
    "            if local_tree_size < 1:\n",
    "                local_tree_size = 1  # local tree size should be at least 1\n",
    "             \n",
    "            # get local model\n",
    "            rf_local = RandomForestRegressor(bootstrap = self.bootstrap, n_estimators = local_tree_size, max_features = self.mtry, random_state = self.random_seed) # input\n",
    "            rf_local.fit(local_X_train, local_y_train)\n",
    "            \n",
    "            # key for storing local rf model in a dictionary\n",
    "            rf_local_key = str(record_index.iloc[i])+\"|\"+ str(coords_array[i][0])+\"|\"+str(coords_array[i][1])\n",
    "            self.local_models[rf_local_key] = rf_local\n",
    "            \n",
    "    \n",
    "    # the function for making predictions using the GRF model\n",
    "    # param X_test contains a data frame of the independent variables in the test dataset\n",
    "    # param coords contains a data frame of the two-dimensional coordinates\n",
    "    def predict(self, X_test, coords_test): \n",
    "        \n",
    "        # first, make prediction using the global RF model \n",
    "        predict_global = self.global_model.predict(X_test).flatten() # get the global predict y first\n",
    "        \n",
    "        # Second, make prediction using the local RF model \n",
    "        coords_test_array = np.array(coords_test, dtype = np.float64)\n",
    "        distance_matrix_test_to_train = distance.cdist(coords_test_array, self.train_data_coords, 'euclidean')\n",
    "        predict_local = []\n",
    "        \n",
    "        for i in range(len(X_test)):\n",
    "            distance_array = distance_matrix_test_to_train[i]\n",
    "            idx = np.argpartition(distance_array, self.local_model_num)  # Get the index of the geographic features that are the nearest to the target geographic feature\n",
    "            idx = idx[:self.local_model_num]\n",
    "            \n",
    "            this_local_prediction = 0\n",
    "            for this_idx in idx:\n",
    "                local_model_key = str(self.train_data_index.iloc[this_idx])+\"|\"+ str(self.train_data_coords[this_idx][0])+\"|\"+str(self.train_data_coords[this_idx][1])\n",
    "                local_model = self.local_models[local_model_key]\n",
    "                this_local_prediction += local_model.predict(X_test[i:i+1]).flatten()[0]\n",
    "            \n",
    "            this_local_prediction = this_local_prediction*1.0 / self.local_model_num  # average local predictions\n",
    "            predict_local.append(this_local_prediction)\n",
    "          \n",
    "        \n",
    "        # Third, combine global and local predictions\n",
    "        predict_combined = []\n",
    "        for i in range(len(predict_global)):\n",
    "            this_combined_prediction = predict_local[i]*self.local_weight + predict_global[i]*(1-self.local_weight) \n",
    "            predict_combined.append(this_combined_prediction)\n",
    "        \n",
    "        \n",
    "        return predict_combined, predict_global, predict_local   # return three types of predictions\n",
    "    \n",
    "    \n",
    "    # this function outputs the local feature importance based on the local models\n",
    "    def get_local_feature_importance(self):\n",
    "        if self.local_models == None:\n",
    "            print(\"The model has not been trained yet...\")\n",
    "            return None\n",
    "        \n",
    "        column_list = [self.train_data_index.name] \n",
    "        for column_name in self.train_data_columns: \n",
    "            column_list.append(column_name) \n",
    "            \n",
    "        feature_importance_df = pd.DataFrame(columns = column_list) \n",
    "        \n",
    "        for model_key in self.local_models.keys():\n",
    "            model_info = model_key.split(\"|\")\n",
    "            this_local_model = self.local_models[model_key]\n",
    "            this_row = {}\n",
    "            this_row[self.train_data_index.name] = model_info[0] # the index of a row\n",
    "            for feature_index in range(0, len(self.train_data_columns)):\n",
    "                this_row[self.train_data_columns[feature_index]]=this_local_model.feature_importances_[feature_index]\n",
    "            \n",
    "            feature_importance_df = feature_importance_df.append(this_row, ignore_index = True) # TypeError: Can only append a dict if ignore_index=True\n",
    "            \n",
    "            \n",
    "        return feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162d4a7",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01f012a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1995"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_compl = pd.read_csv(\"../02 Dataset/05 Dataset 3/Complete_NYC.csv\") #input\n",
    "ct_shp = gpd.read_file(\"../02 Dataset/07 Coordinates info for GWR/NYC_CDC data_Tract_Ob_pro.shp\") # input\n",
    "ct_shp['GEOID'] = ct_shp['GEOID'].astype('int64')\n",
    "X_compl_1 = X_compl.merge(ct_shp, left_on = 'GEOID', right_on = 'GEOID', how = 'left')\n",
    "X_compl_2 = X_compl_1.set_index('GEOID')\n",
    "Y_2 = X_compl_2.pop('obesity_cr')\n",
    "del X_compl_2['geometry']\n",
    "len(X_compl_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1871b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['% Black',\n",
       " '% Ame Indi and AK Native',\n",
       " '% Asian',\n",
       " '% Nati Hawa and Paci Island',\n",
       " '% Hispanic or Latino',\n",
       " '% male',\n",
       " '% married',\n",
       " '% age 18-29',\n",
       " '% age 30-39',\n",
       " '% age 40-49',\n",
       " '% age 50-59',\n",
       " '% age >=60',\n",
       " '% <highschool',\n",
       " 'median income',\n",
       " '% unemployment',\n",
       " '% below poverty line',\n",
       " '% food stamp/SNAP',\n",
       " 'median value units built',\n",
       " 'median year units built',\n",
       " '% renter-occupied housing units',\n",
       " 'population density',\n",
       " 'fafood',\n",
       " 'fitness',\n",
       " 'park',\n",
       " 'Lonpro',\n",
       " 'Latpro']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X_compl_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dda9ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_data(data, stats):\n",
    "    return (data - stats['mean'])/ stats['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b943fca1",
   "metadata": {},
   "source": [
    "# GRF 10K-Fold local_w = 1, local_model_num = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3cf1972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flod: 1\n",
      "flod: 2\n",
      "flod: 3\n",
      "flod: 4\n",
      "flod: 5\n",
      "flod: 6\n",
      "flod: 7\n",
      "flod: 8\n",
      "flod: 9\n",
      "flod: 10\n"
     ]
    }
   ],
   "source": [
    "y_rf_compl_predict = []\n",
    "y_true = []\n",
    "dfs = []\n",
    "\n",
    "ten_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "i = 1\n",
    "\n",
    "for train_index, test_index in ten_fold.split(X_compl_2):\n",
    "    print(\"flod:\", str(i))\n",
    "\n",
    "    X_train_1, X_test_1 = X_compl_2.iloc[train_index], X_compl_2.iloc[test_index]\n",
    "    y_train, y_test = Y_2.iloc[train_index], Y_2.iloc[test_index]\n",
    "    X_train = X_train_1[['% Black','% Ame Indi and AK Native','% Asian','% Nati Hawa and Paci Island','% Hispanic or Latino','% male','% married','% age 18-29','% age 30-39','% age 40-49','% age 50-59','% age >=60','% <highschool','median income','% unemployment','% below poverty line','% food stamp/SNAP','median value units built','median year units built','% renter-occupied housing units','population density','fafood','fitness','park']]\n",
    "    X_test = X_test_1[['% Black','% Ame Indi and AK Native','% Asian','% Nati Hawa and Paci Island','% Hispanic or Latino','% male','% married','% age 18-29','% age 30-39','% age 40-49','% age 50-59','% age >=60','% <highschool','median income','% unemployment','% below poverty line','% food stamp/SNAP','median value units built','median year units built','% renter-occupied housing units','population density','fafood','fitness','park']]\n",
    "    xy_coord = X_train_1[[\"Lonpro\",\"Latpro\"]]\n",
    "    train_index_1 = X_train.index\n",
    "    train_index = pd.Series(train_index_1)\n",
    "    coords_test = X_test_1[[\"Lonpro\",\"Latpro\"]]\n",
    "    \n",
    "    training_stat = X_train.describe().transpose()\n",
    "    scaled_X_train = standarize_data(X_train, training_stat)\n",
    "    scaled_X_test = standarize_data(X_test, training_stat)\n",
    "    \n",
    "    grf = GeographicalRandomForest(560, 7, 220, 1, local_model_num = 38) \n",
    "    grf.fit(scaled_X_train, y_train, xy_coord, train_index)\n",
    "    \n",
    "    predict_combined, predict_global, predict_local = grf.predict(scaled_X_test,coords_test)\n",
    "    \n",
    "    local_feature_importance = grf.get_local_feature_importance()\n",
    "    # local_feature_importance.to_csv(\"../02 Dataset/09 GRF Local Importance/01 NYC/lfi_{}.csv\".format(i))\n",
    "    dfs.append(local_feature_importance)\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "    y_rf_compl_predict = y_rf_compl_predict + predict_combined\n",
    "    y_true = y_true + y_test.tolist()\n",
    "    \n",
    "local_feature_importance1 = pd.concat(dfs)\n",
    "local_feature_importance2 = local_feature_importance1[['GEOID','fafood','fitness','park']]\n",
    "local_feature_importance_last = local_feature_importance2.groupby([\"GEOID\"]).agg({\"fafood\":\"mean\",\"fitness\":\"mean\",\"park\":\"mean\"}).reset_index()\n",
    "local_feature_importance_last.to_csv('../02 Dataset/09 GRF Local Importance/local_feature_importance_NYC.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97cd3c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of the RF model with all predictors: 1.5062633587496983\n",
      "R2 of the RF model with all predictors: 0.9344988020392302\n"
     ]
    }
   ],
   "source": [
    "rf_complete_rmse = mean_squared_error(y_true , y_rf_compl_predict, squared=False) #False means return RMSE value\n",
    "rf_complete_r2 = r2_score(y_true, y_rf_compl_predict)\n",
    "# sociodemographic - estimators\n",
    "print(\"RMSE of the RF model with all predictors: \"+str(rf_complete_rmse))\n",
    "print(\"R2 of the RF model with all predictors: \"+str(rf_complete_r2)) # For R2, I took this one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
